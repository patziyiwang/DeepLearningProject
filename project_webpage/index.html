<!DOCTYPE html>
<html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Deep Learning Class Project
  | Georgia Tech | Fall 2018: CS 4803 / 7643</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>

<link href="css/bootstrap-responsive.min.css" rel="stylesheet">
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name -->
<h1>Obstacle Dynamics Prediction</h1>
<span style="font-size: 20px; line-height: 1.5em;"><strong>Liexiao Ding, Hui Li, Ziyi Wang, Zhanzhan Zhao</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2018 CS 4803 / 7643 Deep Learning: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!-- Goal -->
<h2>Abstract</h2>

  The problem that we try to tackle in this project is obstacle movement prediction, which is essential in research on autonomous driving with only visual information. We split the problem into two parts: first identifying and tracking the obstacle by drawing a bounding box around it, and then predicting the movement of the bounding box instead of the whole image.
<br><br>
<!-- figure -->
<h2>Teaser Figure</h2>
The flowchart below shows the network structure with an input image to identify, track and predict of the moving bounding box. (This one is from <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">AlexNet</a>.)
<br><br>
<!-- Main Illustrative Figure -->
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="images/alexnet.png">
</div>

<br><br>
<!-- Introduction -->
<h2>Introduction / Background / Motivation</h2>
<h4>What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no jargon.</h4>
  In this project, we work on identifying, tracking and predicting the moving obstacles captured in a video. We try to do so by first applying an object detector and tracker which draw bounding boxes around moving obstacles of interest, and then predict the movements and shrinking/expanding of the bounding boxes. We use videos taken by a car-mounted camera and other cars in the video are viewed as moving obstacles to draw boxes from.

<h4>How is it done today, and what are the limits of current practice?</h4>

  Most of the applications in obstacle detection/avoidance for autonomous driving rely on various sensors like LIDAR, sereo cameras, GPS, etc, which are expensive compared with cameras. Moreover, in scenarios like driving off road, it might be infeasible to always access sensors like GPS. 
  <br>
  In the field of video prediction research, Most efforts have been drawn on predicting the entire images. However, predicting videos far into the future in real time has not been very successful, even with advanced network architecture and training scheme (<a href="https://arxiv.org/abs/1706.08033/">[1]</a>, <a href="https://arxiv.org/abs/1511.05440/">[2]</a>, <a href="https://arxiv.org/abs/1605.08104/">[3]</a>). A recent paper in video prediction <a href="https://arxiv.org/abs/1704.05831/">[4]</a> uses attention where only certain areas in an image are predicted while the rest remains still. The limit of this approach is the assumption of a still background which does not hold in autonomous driving setting.
  <br>


<h4>Who cares? If you are successful, what difference will it make?</h4>
  The success of this project is important for future research on obstacle avoidance in autonomous driving with only visual information. The output of the pipeline resulted from this project can be postprocessed by another module to estimate the obstacle location and size, which can then be used by a path planner. This would allow an autonomous driving framework requiring only visual information from a single camera.
  <br>
  Moreover, in the same spirit of <a href="https://arxiv.org/abs/1704.05831/">[4]</a>, we try to predict the moving object, but by using an object tracker to create the mask we are able to avoid the background problem which has to be still. So it can also a meaningful combination in video prediction research.

<br><br>
<!-- Approach -->
<h2>Approach</h2>

  We broke the problem into two parts: obstacle detection/tracking and obstacle dynamics prediction. The tracking part takes in an input image stream and outputs the bounding box information on one or more objects. For this part we turned to existing obstacle detection algorithms since it is a rather established field and there are many existing algorithms that achieve good performance on our objective. The bounding box information is used by the obstacle dynamics prediction part to create a mask, which is then fed into the network for prediction of future masks. For prediction we used a custom encoder-decoder LSTM network structure, while drawing inspirations from existing work on training.
  <h3>Tracking</h3>
  <ul>
  <h4>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</h4>

  We first tried a set of state-of-the-art object tracker: AdaBoost, Multiple Instance Learning, Kernelized Correlation Filters, and GOTURN, which all performed well with only slight occlusion. But to make the tracker more stable, we finally chose <a href="https://arxiv.org/abs/1611.09224">ECO: Efficient Convolution Operators for Tracking</a>, which out performed all the trackers we had tried previously and satisfied our requirement for box tracking. 
  

  <h4>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</h4>
  To get a more stable object box tracking algorithm, we turned to more recent deep learning techniques, and found <a href="https://arxiv.org/abs/1611.09224">ECO</a> out performs all the trackers and satisfies our requirement for video prediciton. 
<br>
Eco addresses both problems of computational complexity and over-fitting. The integration of high-dimensional feature map leads to a radical increase in the number of appearance model parameters, which can easily cause the over-fitting problem. ECO uses the Factorized Convolution Operator to reduce the learning complexity, that is, it efficiently selects the filters with high energy so that there is no need to learn too many parameters for the model. 


</ul>

  <h3>Video Prediction of the Moving Obstacles</h3>
<ul>
  <h4>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</h4>
  Since the idea to predict the video of the obstcales in real time is a fresh idea. The data preprocess, as well as the network structure are all proposed and developed by us. The following shows the details of the code development. 
<br>
  Data-Preprocess: Since only the movement of the bounding box is of interest, to greatly improve the training time, the image data was pre-processed as follows: we first extracted the bounding box pixel knowledge in each image and created a mask the same size of the original image with zeros everywhere except for inside the bounding boxes areas, where the values were set to 1. The masks were keeping shrunken by factors of 2 until a dimension size becomes odd or below 32. All the masks were then used to build a dataset of sequences. The sequence length was a hyperparameter and also a performance indicator. Each mask was flattened before fed into the network. 
  <br>
  Design of the video prediction network structure: the network structure is based on LSTM, since the temporal evolution of the masks is tried to be predicted. The network takes in the flattened masks and outputs a mask of the same size at the next frame. The hyperparameters related to the network are hidden state dimensions, number of LSTM layers and dropout probability. 
 <br>
Moreover, three different techniques are tried to get better results. The first technique is Teacher Forcing, where we always use the true mask as the input to the network to get prediction of future steps of images. For the second approach,  inspired by <a href="https://arxiv.org/abs/1704.05831/">[4]</a>, we utilize an encoder-decoder framework on top of the LSTM, where we provide the network with true masks for a certain number of frames and then use the last hidden state which contains the encoded information to predict another number of frames. We firstly follows <a href="https://arxiv.org/abs/1704.05831/">[4]</a>, where only a dummy input of zeros are as the input to predict the future masks, in which case only the hidden state information is used for prediction. Not being able to get ideal results, we modified the framework by using the predicted mask as input to the next step. In this case we introduced one more hyperparameter in the fraction of the sequence used for prediction.
  <br>
  We tried both L1 and MSE loss, and they resulted in different behaviors in outputs. Adam with weight decay was used as the optimizer. The gradient was also clipped to prevent the exploding gradient problem common in recurrent networks.

  <h4>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</h4>
  The main problem we expected is exploding gradient. We did encounter this problem and found that it is closely related to the hidden state size. Hidden state size too large resulted in normal prediction in the first few batches and NaN after gradient exploded. To tackle this problem we tried different weight and initial hidden state initialization. We found that the default weight initialization worked better than other popular initilization (kaiming_normal, xavier_uniform, xavier_normal), and random normal works better than zeros for hidden state initialization.
  <br>
  Another problem we encountered was a CUDNN problem, which appeared to be related to the number of LSTM layers and batch size. Unfortunately, we couldn't fix the problem with tuning the hyperparameters of LSTM layer number, batch size and hidden state size, and had to limit the number of layers to 1 and batch size to be less than 64.

</ul>
<br><br>

<!-- <h2>ECO: Efficient Convolution Operators for Tracking</h2>
<h3>
  <ol>
  <li>Factorized Convolution Operator</li>
  <li>Generative Sample Space Model</li>
  <li>Model Update Strategy</li>
  </ol>
</h3> -->

<!-- Results -->
<h2>Experiments and Results</h2>

<h4>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</h4>
<h3>Tracking</h3>
<ul>
The following are the visualization of tracker BOOSTING, MIL and TLD:
<br><br>
<img src="./images/tracked_Car1_BOOSTING.gif" alt="BOOSTING">
<img src="./images/tracked_Car1_MIL.gif" alt="MIL">
<img src="./images/tracked_Car1_TLD.gif" alt="TLD">
<br><br>
We trained an ECO tracker and it's the only tracker tested that correctly tracked our target object:
<br><br>
<img src="./images/tracked_Car1_ECO.gif" alt="ECO">
</ul>

<h3>Obstacle Dynamics Prediction</h3>
<ul><h4>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</h4>
    We measured success by the loss and visualization of prediction against true masks. As mentioned previously, we tried using both L1 and MSE (L2) loss in training. In addition, we tried three different training techniques. The following are the loss history for the best tuned network with different loss functions and training techniques:

    We did not manage to achieve the desired results, and there are a few potential reasons and future directions.

</ul>

<!-- Main Results Figure -->
<!-- <div style="text-align: center;">
<img style="height: 300px;" alt="" src="images/results.png"> -->
</div>
<div style="text-align: center;">
<!--=<img style="height: 300px;" alt="" src="images/results.png">-->
<br><br>
  <hr>
  <footer>
  <p>© Liexiao Ding, Hui Li, Ziyi Wang, Zhanzhan Zhao</p>
  </footer>
</div>
</div>

<br><br>

</body></html>
